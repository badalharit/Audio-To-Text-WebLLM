<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Simple Whisper WebGPU Voice to Text</title>
</head>
<body>
    <h1>Voice to Text with Whisper WebGPU</h1>
    
    <button id="startBtn">Start Recording</button>
    <button id="stopBtn" disabled>Stop Recording</button>
    
    <div id="status">Ready</div>
    <div id="transcription"></div>

    <script type="module">
        import { pipeline } from 'https://cdn.jsdelivr.net/npm/@huggingface/transformers@3.0.0';

        let transcriber = null;
        let mediaRecorder = null;
        let audioChunks = [];

        const startBtn = document.getElementById('startBtn');
        const stopBtn = document.getElementById('stopBtn');
        const status = document.getElementById('status');
        const transcription = document.getElementById('transcription');

        // Initialize the Whisper model with WebGPU
        async function initializeModel() {
            status.textContent = 'Loading Whisper model...';
            try {
                transcriber = await pipeline(
                    'automatic-speech-recognition',
                    'onnx-community/whisper-tiny.en',
                    { device: 'webgpu' }
                );
                status.textContent = 'Model loaded! Ready to record.';
                startBtn.disabled = false;
            } catch (error) {
                console.error('Error loading model:', error);
                status.textContent = 'Error loading model. Check console.';
            }
        }

        // Start recording
        async function startRecording() {
            try {
                const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                mediaRecorder = new MediaRecorder(stream);
                audioChunks = [];

                mediaRecorder.ondataavailable = (event) => {
                    audioChunks.push(event.data);
                };

                mediaRecorder.onstop = async () => {
                    const audioBlob = new Blob(audioChunks, { type: 'audio/wav' });
                    const audioUrl = URL.createObjectURL(audioBlob);
                    
                    status.textContent = 'Transcribing...';
                    
                    try {
                        const result = await transcriber(audioUrl);
                        transcription.textContent = result.text;
                        status.textContent = 'Transcription complete!';
                    } catch (error) {
                        console.error('Transcription error:', error);
                        status.textContent = 'Transcription failed. Check console.';
                    }
                    
                    URL.revokeObjectURL(audioUrl);
                };

                mediaRecorder.start();
                status.textContent = 'Recording... Click Stop when done.';
                startBtn.disabled = true;
                stopBtn.disabled = false;

            } catch (error) {
                console.error('Error starting recording:', error);
                status.textContent = 'Error accessing microphone.';
            }
        }

        // Stop recording
        function stopRecording() {
            if (mediaRecorder && mediaRecorder.state === 'recording') {
                mediaRecorder.stop();
                mediaRecorder.stream.getTracks().forEach(track => track.stop());
                startBtn.disabled = false;
                stopBtn.disabled = true;
            }
        }

        // Event listeners
        startBtn.addEventListener('click', startRecording);
        stopBtn.addEventListener('click', stopRecording);

        // Initialize when page loads
        initializeModel();
    </script>
</body>
</html>
